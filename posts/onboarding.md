# Table of Contents
- [Table of Contents](#table-of-contents)
  - [Onboarding](#onboarding)
  - [Tokenizer](#tokenizer)
  - [Parser](#parser)
    - [Token Index](#token-index)
    - [Matching text](#matching-text)
    - [Utility methods](#utility-methods)
    - [Command fallback](#command-fallback)
    - [Expression parsing](#expression-parsing)
  - [Generator](#generator)
    - [Generating queries](#generating-queries)
    - [Utility methods](#utility-methods-1)
    - [Pretty print](#pretty-print)
  - [Schema](#schema)
  - [Optimizer](#optimizer)
    - [Optimization rules](#optimization-rules)
      - [Qualify](#qualify)
        - [Identifier normalization](#identifier-normalization)
        - [Qualifying tables \& columns](#qualifying-tables--columns)
      - [Type annotation](#type-annotation)
  - [Dialects](#dialects)
    - [Implementing a custom dialect](#implementing-a-custom-dialect)
  - [Column Level Lineage](#column-level-lineage)
    - [Implementation details](#implementation-details)

## Onboarding
This document aims to familiarize the reader with SQLGlot's codebase & architecture.

Generally, a background in programming languages / compilers as well as database internals is required. A good starting point for the former (which also served as the foundation for SQLGlot) is [Crafting Interpreters](https://craftinginterpreters.com/) by Robert Nystrom.

## Tokenizer

The tokenizer module (`tokens.py`) is responsible for breaking down SQL code into tokens, which are the smallest units of meaningful data (like keywords, identifiers, literals, operators, etc.). This process is also commonly referred to as lexing/lexical analysis.

SQLGlot aims to maintain a [mirroring Rust version](https://tobikodata.com/sqlglot-jumps-on-the-rust-bandwagon.html) as well in `sqlglotrs/tokenizer.rs` for a more efficient tokenization.

> [!IMPORTANT]
> Changes in one (python / rust) implementation must be reflected on the other one (rust / python) as well.

An example of the Tokenizer:

```python
from sqlglot import tokenize

tokens = tokenize("SELECT b FROM table WHERE c = 1")
for token in tokens:
    print(token)

# <Token token_type: <TokenType.SELECT: 'SELECT'>, text: 'SELECT', line: 1, col: 6, start: 0, end: 5, comments: []>
# <Token token_type: <TokenType.VAR: 'VAR'>, text: 'b', line: 1, col: 8, start: 7, end: 7, comments: []>
# <Token token_type: <TokenType.FROM: 'FROM'>, text: 'FROM', line: 1, col: 13, start: 9, end: 12, comments: []>
# <Token token_type: <TokenType.TABLE: 'TABLE'>, text: 'table', line: 1, col: 19, start: 14, end: 18, comments: []>
# <Token token_type: <TokenType.WHERE: 'WHERE'>, text: 'WHERE', line: 1, col: 25, start: 20, end: 24, comments: []>
# <Token token_type: <TokenType.VAR: 'VAR'>, text: 'c', line: 1, col: 27, start: 26, end: 26, comments: []>
# <Token token_type: <TokenType.EQ: 'EQ'>, text: '=', line: 1, col: 29, start: 28, end: 28, comments: []>
# <Token token_type: <TokenType.NUMBER: 'NUMBER'>, text: '1', line: 1, col: 31, start: 30, end: 30, comments: []>
```

The query is split into a stream of keywords and for each matched pattern the appropriate token is generated. Each token is generally described by its `TokenType` and by its value stored in the `text` field. Other metadata such as `line` or `col` are stored as well during this step and can help with e.g. error diagnostics.

The `TokenType` enum provides an indirection layer between the text of each token and its type. For example, `!=` and `<>` are often used interchangeably as "not equals", so to group these common tokens together one can map them both to `TokenType.NEQ` in the appropriate dictionary `Tokenizer.KEYWORDS`.



## Parser

The parser module takes the list of tokens generated by the tokenizer and builds an [abstract syntax tree (AST)](https://en.wikipedia.org/wiki/Abstract_syntax_tree). As the AST is the core concept of SQLGlot, a more detailed tutorial on its representation, traversal & mutation can [be found here](https://github.com/tobymao/sqlglot/blob/main/posts/ast_primer.md).

The main goal of the base parser is to capture as many overlapping constructs from different dialects as possible; this leniency is what allows SQLGlot to be expressive without repeating itself.

> [!WARNING]
> SQLGlot does not aim to be a SQL validator, so it may not detect certain syntax errors.

Each Parser class defines the following:

- A set of flags such as `SUPPORTS_IMPLICIT_UNNEST` or `SUPPORTS_PARTITION_SELECTION`, which are used to control the parsing behavior of each dialect, avoiding code duplication.

- A set of `token -> Callable` mappings, divided semantically in the appropriate dictionaries. When the parser comes across the `token` (string or `TokenType`) on the left-hand side it invokes the mapping function to initialize the corresponding Expression / AST node. An example of that is the `FUNCTIONS` dictionary which will build the appropriate `exp.Func` node depending on the string key:

```Python3
 FUNCTIONS: t.Dict[str, t.Callable] = {
    …,
    "LOG2": lambda args: exp.Log(this=exp.Literal.number(2), expression=seq_get(args, 0)),
    "LOG10": lambda args: exp.Log(this=exp.Literal.number(10), expression=seq_get(args, 0)),
    "MOD": build_mod,
    …,
```

### Token Index
The token index/cursor is a crucial part of the parser, keeping track of the current position within the list of tokens.

Once the current token has been successfully processed, the parser can move on to the next one in the sequence through `_advance()` which increments the index.

If a list of tokens has been consumed but an error state has been reached, the parser can move backwards through `_retreat()` which backtracks by decreasing the index.

### Matching text
When parsing an SQL statement the parser needs to identify the specific keywords to correctly build the AST. For instance, in order to parse a window specification the `OVER` token is matched first, then the partition clause is matched by consuming the `PARTITION`, `BY` tokens sequentially, then the partition expression is parsed etc.

As this is the fundamental process of the parser, the family of `_match` methods aim to facilitate this by offering the necessary abstractions. The most notable methods are:


Method                | Use
-------------------------|-----
**`_match(type)`**       | Attempt to match a single token with a specific `TokenType`
**`_match_set(types)`**  | Attempt to match a single token from a set of `TokenType`s
**`_match_text_seq(texts)`**   | Attempt to match a continuous sequence of strings/texts
**`_match_texts(texts)`**   | Attempt to match a keyword from a set of strings/texts



### Utility methods
A list of helper methods which aid in various parsing tasks are also implemented and their use is encouraged to reduce code duplication and manual token/text matching, such as:

Method                | Use
-------------------------|-----
**`_parse_csv(parse_method)`**  | Attempt to parse comma-separated values using the appropriate `parse_method` callable
**`_parse_wrapped(parse_method)`**  | Attempt to parse a specific construct that is (optionally) wrapped in parentheses
**`_parse_wrapped_csv(parse_method)`**  | Attempt to parse comma-separated values that are (optionally) wrapped in parentheses


### Command fallback
Certain SQL statements require significant effort & parsing complexity to fully capture which is magnified by the fact that each dialect defines its own specification. Without a fallback mechanism, each feature added to the parser would have to work correctly & be maintained across all supported dialects, which significantly hinders incremental development.

Thus, a common pattern in the parser (provided a certain path can't be matched) is to fallback to a command (`exp.Command` node), meaning that the SQL input is stored as is, without being actually parsed.. Even though this behavior limits the semantic understanding of the query and its ability to be transpiled to other dialects, it also allows the parser to continue operating while additional support for these "missed paths" is in development.


## Generator
The generator module is responsible for converting the abstract syntax tree (AST) back into SQL (query string).

Each generator class defines the following:

- A set of flags such as `ALTER_TABLE_INCLUDE_COLUMN_KEYWORD`. As with the parser, these flags can alter the behavior of the Generator with the intention of eliminating the need for repeated code between Generator implementations.

- A set of `Expression -> str` mappings, divided semantically in the appropriate dictionaries. The generator traverses the AST recursively and for each node that it visits, it produces the equivalent string part. This can be thought of as the reverse operation of the parser, where expressions / AST nodes are converted back to strings.


The aforementioned mappings can be added as:
- Entries in the `TRANSFORMS` dictionary, which are best suited for single-line generations.
- Functions that are named as `<expr_name>_sql(...)`. For example, to generate the node `exp.SessionParameter` we can directly append the function `def sessionparameter_sql(...)` in the generator class

### Generating queries
The key method in the Generator module is the `sql()` function which is responsible for generating the string representation of each `Expression`.

It essentially searches for the mapping `Callable` by probing both the `TRANSFORMS` dictionary as well as the `Generator`'s attributes for the existance of an `<exprname>_sql()` method, which then proceeds to invoke.

### Utility methods

The other helper methods of the Generator offer quality-of-life abstractions over the `sql()` method such as:

Method                | Use
-------------------------|-----
**`expressions()`**  | Generates the string representation for a list of `Expression`s, employs a series of arguments to help with their formatting
**`func()`, `rename_func()`**  | Given a function name and an `Expression`, outputs the string representation of a function call by generating the expression's args as the func args.

### Pretty print
Pretty printing refers to the process of generating formatted & consistently styled SQL which improves readability, debugging, code reviewing etc.

It is up to the developer to ensure that complex constructs are properly indented, both by whitespaces and newlines; To aid in that process, the Generator class offers the `sep()` and `seg()` helper methods.

## Dialects

SQLGlot supports multiple SQL dialects, allowing for the parsing, transformation, and generation of SQL queries specific to various database systems. A SQL dialect refers to the variations and specific syntax used by different SQL database systems (e.g., PostgreSQL, MySQL, SQLite, Oracle, etc.). Each dialect may have unique features, reserved keywords, and syntactical differences.

Even though there is a SQL standard, most SQL engines support a variation of that standard (their own "dialect") which makes it difficult to write portable SQL. SQLGlot attempts to bridge all the different variations in a "superset" dialect,  (often called the _base_ or _sqlglot_ dialect).

There is a Dialect subclass for every supported SQL dialect, whose Tokenizer, Parser and Generator components are extended as needed. Dialect definitions can be found under `dialects/<dialect>.py`. The base dialect components are defined within `tokens.py`, `parser.py` and `generator.py`, respectively.

When adding features that exist in many engines, it's best to factor out the common/overlapping parts in the _base_ dialect respecting the DRY principle. Any dialect-specific features that cannot (or should not) be repeated can then be added in the appropriate subclass.

The Dialect class also contains a set of flags that are visible to its tokenizer, parser and generator. Flags are usually added in a Dialect when they need to be visible by at least two components, in order to avoid code duplication.
### Implementing a custom dialect
Creating a new SQL dialect may seem complicated at first, but it is actually quite simple in SQLGlot:

```Python
from sqlglot import exp
from sqlglot.dialects.dialect import Dialect
from sqlglot.generator import Generator
from sqlglot.tokens import Tokenizer, TokenType


class Custom(Dialect):
    class Tokenizer(Tokenizer):
        QUOTES = ["'", '"']  # Strings can be delimited by either single or double quotes
        IDENTIFIERS = ["`"]  # Identifiers can be delimited by backticks

        # Associates certain meaningful words with tokens that capture their intent
        KEYWORDS = {
            **Tokenizer.KEYWORDS,
            "INT64": TokenType.BIGINT,
            "FLOAT64": TokenType.DOUBLE,
        }

    class Generator(Generator):
        # Specifies how AST nodes, i.e. subclasses of exp.Expression, should be converted into SQL
        TRANSFORMS = {
            exp.Array: lambda self, e: f"[{self.expressions(e)}]",
        }

        # Specifies how AST nodes representing data types should be converted into SQL
        TYPE_MAPPING = {
            exp.DataType.Type.TINYINT: "INT64",
            exp.DataType.Type.SMALLINT: "INT64",
            ...
        }
```

The above example demonstrates how certain parts of the base Dialect class can be overridden to match a different specification. Even though it is a fairly realistic starting point, we strongly encourage the reader to study existing dialect implementations in order to understand how their various components can be modified, depending on the use-case.


## Schema
The representation of a database schema. The file `schema.py` defines the abstract classes `Schema` and `AbstractMappingSchema`; The implementing class is `MappingSchema` which can be defined as a nested dictionary e.g:

```Python
schema = MappingSchema({"t": {"x": "int", "y": "datetime"}})
```

A schema is needed in various SQLGlot modules (most importantly, the optimizer and column level lineage) and is utilized to verify table columns, their types etc.

## Optimizer

The optimizer module in SQLGlot (`optimizer.py`) is responsible for producing canonicalized & efficient SQL queries by applying a series of optimization rules. These optimizations can include simplifying expressions, removing redundant operations, and rewriting queries for better performance. The optimizer operates on the abstract syntax tree (AST) generated by the parser, transforming it into a more optimized form while preserving the semantics of the original query.

> [!NOTE]
> The optimizer does not currently support statistics and thus can't employ cost-based optimizations such as join reordering
>


### Optimization rules
The optimizer essentially applies [a list of rules](https://sqlglot.com/sqlglot/optimizer.html) in a sequential manner, where each rule takes in an AST and produces a canonicalized & optimized version of it.

> [!WARNING]
> The rule order is important as one rule might depend on others to work properly. It is also discouraged to manually apply rules on ASTs which might lead to undefined behavior

The foundational rules which serve to canonicalize an AST and unlock further optimizations are the following:

#### Qualify
The most important rule in the optimizer is the `qualify` step as it is responsible for rewriting the AST such that all tables and columns are _normalized_ and _qualified_.

##### Identifier normalization
The normalization step (`normalize_identifiers.py`) refers to transforming all the unquoted identifiers to lower or upper case (depending on the dialect), essentially making them case-insensitive.

> [!NOTE]
> Some dialects (e.g. BigQuery) treat identifiers as case-insensitive even when they're quoted, so in these cases all identifiers are normalized.

For instance, in Snowflake the normalization strategy (i.e what the unquoted identifiers will be resolved into) is upper case:

```Python
normalize_identifiers(parse_one('SELECT Bar.A AS A FROM "Foo".Bar')).sql()
# 'SELECT bar.a AS a FROM "Foo".bar'

normalize_identifiers("foo", dialect="snowflake").sql(dialect="snowflake")
# 'FOO'
```

##### Qualifying tables & columns
After normalizing the identifiers, all tables and columns are qualified. This means that column names such as "col" become "table"."col", and all table references are bound to an alias.

```Python
schema = {"tbl": {"col": "INT"}}
expression = sqlglot.parse_one("SELECT col FROM tbl")
qualify(expression, schema=schema).sql()

# 'SELECT "tbl"."col" AS "col" FROM "tbl" AS "tbl"'
```

The `qualify` rule also offers a set of subrules that can further canonicalize the query. An example of that is the ability to expand stars such that each `SELECT *` is replaced by a projection list of the referenced columns, e.g building on to the previous example:


```Python
qualify(sqlglot.parse_one("SELECT * FROM tbl"), schema=schema).sql()

# 'SELECT "tbl"."col" AS "col" FROM "tbl" AS "tbl"'
```

#### Type annotation
Traverses the AST and attempts to infer the type of each expression. In many cases, both transpilation and optimization require types to work properly.

For instance, many dialects allow string concatenation through the `+` operator while others only offer this feature through functions such as `CONCAT(s1, s2, …)`. Transpiling an expression such as `a + b` between such dialects is sensitive to the types of `a` and `b` respectively.

Thus, without having proper type annotation across the board, the optimizer wouldn't be able to understand the SQL and employ various optimizations & transformations safely.



## Column Level Lineage
Column-level lineage tracks the flow of data from source columns in one table to target columns in another table through various transformations and operations. It is an important aspect of data lineage that helps in understanding how data is derived, transformed, and used across different parts of a data pipeline or database system.

SQLGlot offers an implementation of CLL in `lineage.py`. An example of its use case:


```Python
from sqlglot.lineage import lineage

node = lineage(
    column="a",
    sql="WITH z AS (SELECT a FROM y) SELECT a FROM z",
    schema={"x": {"a": "int"}},
    sources={"y": "SELECT * FROM x"},
)
```

### Implementation details
In the previous example, we're interested in tracing the origin of column `a`. Before lineage can be traced, the following preparatory steps must be carried out:


1. Expand the _sources_ such that we have a single standalone query:

```SQL
WITH z AS (SELECT a FROM (SELECT * FROM x) AS y) SELECT a FROM z
```

2. Qualify the expanded query:

```SQL
WITH z AS (SELECT y.a FROM (SELECT x.a FROM x AS x) AS y) SELECT z.a FROM z AS z
```

With these steps the query is canonicalized and the lineage can be tracked. This is a top-down process, meaning that the search would start from `z.a` (outer scope) and traverse inwards to find that its origin is `y.a`, which in turn is based on `x.a`. The search continues until the innermost scope has been visited; At this point all `sources` are exhausted so the `schema` will be searched to validate that the final table (e.g `x` in this case) exists and that the target column `a` is defined in it.

At each step, the column of interest (i.e `z.a`, `y.a` etc) is wrapped in a lineage object `Node` that is linked to its upstream (or parent) nodes; This incrementally builds a linked-list of `Nodes` that traces the lineage backwards for the target column such as `x.a -> y.a -> z.a`. The result can be visualized using the `node.to_html()` function which outputs an HTML graph based on vis.js.
