# Table of Contents
- [Table of Contents](#table-of-contents)
  - [SQLGlot Onboarding](#sqlglot-onboarding)
  - [Resources](#resources)
  - [Tokenizer](#tokenizer)
  - [Parser](#parser)
    - [Token Index](#token-index)
    - [Matching text](#matching-text)
    - [Utility methods](#utility-methods)
    - [Command fallback](#command-fallback)
    - [Expression parsing](#expression-parsing)
  - [Generator](#generator)
    - [Utility methods](#utility-methods-1)
    - [Pretty print](#pretty-print)
  - [Schema](#schema)
  - [Optimizer](#optimizer)
    - [Optimization rules](#optimization-rules)
      - [Qualify](#qualify)
        - [Identifier normalization](#identifier-normalization)
        - [Qualifying tables \& columns](#qualifying-tables--columns)
      - [Type annotation](#type-annotation)
  - [Dialects](#dialects)
    - [Implementing a custom dialect](#implementing-a-custom-dialect)
  - [Column Level Lineage](#column-level-lineage)
    - [Implementation details](#implementation-details)

## SQLGlot Onboarding
This is a document describing SQLGlot's architecture & contribution best-practices.


## Resources

- General
  - https://craftinginterpreters.com/
- Optimizer
  -  Unnest Subqueries: https://duckdb.org/2023/05/26/correlated-subqueries-in-sql.html


## Tokenizer

The tokenizer module (`tokens.py`) is responsible for breaking down SQL queries into tokens, which are the smallest units of meaningful data (like keywords, identifiers, literals, operators, etc.). This process is also commonly referred to as lexing/lexical analysis.

SQLGlot aims to maintain a [mirroring Rust version](https://tobikodata.com/sqlglot-jumps-on-the-rust-bandwagon.html) as well found in `sqlglotrs/tokenizer.rs` for a more efficient tokenization.

> [!IMPORTANT]
> Changes in one (python / rust) implementation must be reflected on the other one (rust / python) as well.

An example of the Tokenizer:

```python
from sqlglot.tokens import Tokenizer

tokenizer = Tokenizer()
tokens = tokenizer.tokenize("SELECT b FROM table WHERE c = 1")
for token in tokens:
    print(token)

# <Token token_type: <TokenType.SELECT: 'SELECT'>, text: 'SELECT', line: 1, col: 6, start: 0, end: 5, comments: []>
# <Token token_type: <TokenType.VAR: 'VAR'>, text: 'b', line: 1, col: 8, start: 7, end: 7, comments: []>
# <Token token_type: <TokenType.FROM: 'FROM'>, text: 'FROM', line: 1, col: 13, start: 9, end: 12, comments: []>
# <Token token_type: <TokenType.TABLE: 'TABLE'>, text: 'table', line: 1, col: 19, start: 14, end: 18, comments: []>
# <Token token_type: <TokenType.WHERE: 'WHERE'>, text: 'WHERE', line: 1, col: 25, start: 20, end: 24, comments: []>
# <Token token_type: <TokenType.VAR: 'VAR'>, text: 'c', line: 1, col: 27, start: 26, end: 26, comments: []>
# <Token token_type: <TokenType.EQ: 'EQ'>, text: '=', line: 1, col: 29, start: 28, end: 28, comments: []>
# <Token token_type: <TokenType.NUMBER: 'NUMBER'>, text: '1', line: 1, col: 31, start: 30, end: 30, comments: []>
```

The query is split in a stream of keywords and for each matched pattern the appropriate token is generated. Each token is generally described by it's `TokenType` and by it's value stored in the `text: <payload>` field. Other metadata such as `line` or `col` are stored as well and can help with e.g. error diagnostics.

The `TokenType` enum provides an indirection layer between the text of each token and it's type. For example, `!=` and `<>` are often used interchangeably as "not equals", so to group these common tokens together we can map them both to `TokenType.NEQ` in the appropriate dictionary `Tokenizer.KEYWORDS`.



- TODO: How to define a tokentype. Note that it is discouraged?

## Parser

The parser module takes the list of tokens generated by the tokenizer and builds an [abstract syntax tree (AST)](https://en.wikipedia.org/wiki/Abstract_syntax_tree). As the AST is the core concept of SQLGlot, a more detailed tutorial on it's representation, traversal & mutation can [be found here](https://github.com/tobymao/sqlglot/blob/main/posts/ast_primer.md).

The main goal of the base parser is to capture as many overlapping constructs from different dialects as possible; this leniency is what allows SQLGlot to be expressive without repeating itself.

> [!WARNING]
> SQLGlot does not aim to be a strict query validator, so semantically incorrect queries have undefined behavior.

Each Parser class defines the following:

- A set of flags such as `SUPPORTS_IMPLICIT_UNNEST` or `SUPPORTS_PARTITION_SELECTION`. This is done in order to control the behavior of one base class function across many parsers depending on the value of the flag.

- A set of `token -> Callable` mappings, divided semantically in the appropriate dictionaries. When the parser comes across the `token` (string or `TokenType`) on the left-hand side it invokes the mapping function to initialize the corresponding Expression / AST node. An example of that is the `FUNCTIONS` dictionary which will build the appropriate `exp.Func` node depending on the string key.

### Token Index
The token index/cursor is a crucial part of the parser, keeping track of the current position within the list of tokens.

Once the current token has been successfully processed, the parser can move on to the next one in the sequence through `_advance()` which increments the index.

If a list of tokens has been consumed but an error state has been reached, the parser can move backwards through `_retreat()` which backtracks by decreasing the index.

### Matching text
When parsing an SQL statement the parser needs to identify the specific keywords to correctly build the AST. For instance, in order to parse a window specification the `OVER` token is matched first, then the partition clause is matched by consuming the `PARTITION`, `BY` tokens sequentially, then the partition expression is gathered etc.

As this is the fundamental process of the parser, the family of `_match` methods aim to facilitate this by offering the neccessary abstractions. The most notable methods are:


Method                | Use
-------------------------|-----
**`_match(type)`**       | Attempt to match a single token with a specific `TokenType`
**`_match_set(types)`**  | Attempt to match a single token from a set of `TokenType`s
**`_match_text_seq(texts)`**   | Attempt to match a continuous sequence of strings/texts
**`_match_texts(texts)`**   | Attempt to match a keyword from a set of strings/texts



### Utility methods
A list of helper methods which aid in various parsing tasks are also implemented and their use is encouraged to reduce code duplication and manual token/text matching, such as:

Method                | Use
-------------------------|-----
**`_parse_csv(parse_method)`**  | Attempt to parse comma-separated values using the appropriate `parse_method` callable
**`_parse_wrapped(parse_method)`**  | Attempt to parse a specific construct that is (optionally) wrapped in parentheses
**`_parse_wrapped_csv(parse_method)`**  | Attempt to parse comma-separated values that are (optionally) wrapped in parentheses


### Command fallback
Certain SQL statements require significant effort & parsing complexity to fully capture which is magnified by the fact that each dialect defines it's own specification. Without a fallback mechanism, each feature added to the parser would have to work correctly & be maintained across all supported dialects, which significantly hinders incremental development iteration.

Thus, a common pattern in the parser if a certain path can't be matched is to fallback to a command (`exp.Command` node) which captures all the SQL string as-is. Even though that limits the semantic understanding of the query and it's ability to be transpiled to other dialects, it also allows the parser to continue working as expected while additional support for these "missed paths" is in development.


### Expression parsing
The top-level function for parsing expressions is `_parse_expression`. It's grammar is the following:

- TODO: Complete this or throw way, the grammar is getting out of hand

```
<expression> -> <alias> <conjuction>
<conjuction> -> <equality> { ('AND' | 'OR') <equality> }*
<equality>   -> <comparison> { (':=' | '=' | '!=' | '<=>') <comparison> }*
<comparison> -> <range> { ('<' | '<=' | '>' | '=>') <range> }*
<range>      -> <bitwise> | {'NOT'} {}
<bitwise>    -> <term>
<term>       -> <factor>  { ('-' | '%' | '+' ) <factor> }*
<factor>     -> <exponent> | <unary>
<exponent>   -> <unary> { ('^' | '**') <unary> }*
<unary>      -> NOT <equality> | ('-' | '~' | '+' | '|\' | '||\') <unary> | <type>
<type>       -> <interval> | <column>
```

## Generator
The generator module is responsible for converting the abstract syntax tree (AST) back into SQL (query string) for a specific dialect.

Each generator class defines the following:

- A set of flags such as `ALTER_TABLE_INCLUDE_COLUMN_KEYWORD`. As with the parser, these flags can slightly alter the behavior of the Generator with the intetion of eliminating the need for repeated code between Generator implementations.

- A set of `Expression -> str` mappings, divided semantically in the appropriate dictionaries. The generator traverses the AST recursively and for each node that it visits, it produces the equivalent string part. This can be thought of as the reverse operation of the parser, where expressions / AST nodes are converted back to strings.


The aforementioned mappings can be added as:
- Entries in the `TRANSFORMS` dictionary, which are best suited for single-line generations.
- Functions that are named as `<expr_name>_sql(...)`. For example, to generate the node `exp.SessionParameter` we can directly append the function `def sessionparameter_sql(...)` in the generator class

### Utility methods
The key method in the Generator module is the `sql()` function as it is invoked to generate the string representation of each `Expression`. It essentially searches for the mapping `Callable` by probing the `TRANSFORMS` dictionary as well as the `Generator`'s attributes for the existance of `<exprname>_sql()`.

The other helper methods of the Generator offer quality-of-life abstractions over the `sql()` method such as:

- `expressions()`: Generates the string representation for a list of `Expression`s, employs a series of arguments to help with their formatting
- `func()` and `rename_func()`: Given a function name and an `Expression`, outputs the string representation of a function call by generating the expression's args as the func args.

### Pretty print
Pretty printing refers to the process of generating formatted & consistently styled SQL which improves readability, debugging, code reviewing etc.

It is up to the developer to ensure that complex constructs are properly indented, both by whitespaces and newlines; To aid in that process, the Generator class offers helper methods such as `sep()` and `seg()`.

## Schema
The representation of a database schema. The file `schema.py` defines the abstract classes `Schema` and `AbstractMappingSchema`; The implementing class is `MappingSchema` which can be defined as a nested dictionary e.g:

```Python
schema = MappingSchema({"t": {"x": "int", "y": "datetime"}})
```

A schema is needed in various SQLGlot modules (most importantly, the optimizer and column level lineage) and is consolidated to verify table columns, their types etc.

## Optimizer

The optimizer module in SQLGlot (`optimizer.py`) is responsible for improving the efficiency and performance of SQL queries by applying a series of optimization rules. These optimizations can include simplifying expressions, removing redundant operations, and rewriting queries for better performance. The optimizer operates on the abstract syntax tree (AST) generated by the parser, transforming it into a more optimized form while preserving the semantics of the original query.

> [!NOTE]
> The optimizer does not currently support statistics and thus can't employ cost-based optimizations such as join reordering
>


### Optimization rules
The optimizer essentially applies [a list of rules](https://sqlglot.com/sqlglot/optimizer.html) in a sequential manner, where each rule takes in an AST and produces a slightly more canonicalized & optimized version of it.

> [!WARNING]
> The rule order is important as one rule might depend on others to work properly. It is also discouraged to manually apply rules on ASTs which might lead to undefined behavior

The rules which serve as a foundation in unlocking optimizations are the following:

#### Qualify
The most important rule in the optimizer is the `qualify` step as it is responsible for rewriting the AST such that all tables and columns are _normalized_ and _qualified_.

##### Identifier normalization
The normalization step (`normalize_identifiers.py`) refers to transforming all the **unquoted** identifiers to lower or upper case (depending on the dialect), essentially making them case-insensitive.

> [!NOTE]
> Some dialects (e.g. BigQuery) treat identifiers as case-insensitive even when they're quoted, so in these cases all identifiers are normalized.

For instance, in Snowflake the normalization strategy (i.e what the unquoted identifiers will be resolved into) is upper case:

```Python
normalize_identifiers(parse_one('SELECT Bar.A AS A FROM "Foo".Bar')).sql()
# 'SELECT bar.a AS a FROM "Foo".bar'

normalize_identifiers("foo", dialect="snowflake").sql(dialect="snowflake")
# 'FOO'
```

##### Qualifying tables & columns
After having normalized the identifiers, we _qualify_ the columns such that they're prefixed by their table name as well as the tables such that they're bound to an alias:

```Python
schema = {"tbl": {"col": "INT"}}
expression = sqlglot.parse_one("SELECT col FROM tbl")
qualify(expression, schema=schema).sql()

# 'SELECT "tbl"."col" AS "col" FROM "tbl" AS "tbl"'
```

The `qualify` rule also offers a set of subrules that can further canonicalize the query. An important example of that is the ability to expand stars such that each `SELECT *` is replaced by a projection list of the referenced columns, e.g building on to the previous example:


```Python
qualify(sqlglot.parse_one("SELECT * FROM tbl"), schema=schema).sql()

# 'SELECT "tbl"."col" AS "col" FROM "tbl" AS "tbl"'
```

#### Type annotation
Traverses the AST and attempts to infer the type of each expression. Many sophisticated optimizations require types in order to work in various scenarios; For instance, many dialects offer an `UNNEST` function (or flavors of it) which can be used to expand/flatten expressions of various types such as arrays and structs.

The semantics of the function can be easily inferred when the input is a literal of that type e.g. `UNNEST([1, 2])` is clearly an _array_ expansion, but in most real-world cases the inputs to these functions originate from more complex expressions, identifiers, columns etc. Thus, without having proper type annotation across the board, the optimizer wouldn't be able to understand the SQL and employ various optimizations safely.


## Dialects

SQLGlot supports multiple SQL dialects, allowing for the parsing, transformation, and generation of SQL queries specific to various database systems. A SQL dialect refers to the variations and specific syntax used by different SQL database systems (e.g., PostgreSQL, MySQL, SQLite, Oracle, etc.). Each dialect may have unique features, reserved keywords, and syntactical differences.

Even though there is a SQL standard, most SQL engines support a variation of that standard (their own "dialect") which makes it difficult to write portable SQL. SQLGlot attempts to bridge all the different variations in a "superset" dialect (often called _base_ or _sqlglot_ dialect) spanning that set.

Each SQL variation has its own Dialect subclass (found in `dialects/<dialect>.py`, extending the corresponding Tokenizer, Parser and Generator classes as needed. The base dialect is implemented by the aforementioned modules in `tokens.py`, `parser.py`, `generator.py`.

When adding features that exist in many engines, it's best to factor out the common/overlapping parts in the _base_ dialect respecting the DRY principle. Any dialect-specific features that cannot (or should not) be repeated can then be added in the appropriate subclass.

As with the previous modules, the Dialect class defines a set of flags that span all the submodules (tokenizer, parser, generator). Such flags should be added in the Dialect class only if they're used by multiple submodules i.e by both the parser and generator, otherwise they're best fit to their specific module.

### Implementing a custom dialect
Creating a new SQL dialect may seem complicated at first, but it is actually quite simple in SQLGlot:

```Python
from sqlglot import exp
from sqlglot.dialects.dialect import Dialect
from sqlglot.generator import Generator
from sqlglot.tokens import Tokenizer, TokenType


class Custom(Dialect):
    class Tokenizer(Tokenizer):
        QUOTES = ["'", '"']  # Strings can be delimited by either single or double quotes
        IDENTIFIERS = ["`"]  # Identifiers can be delimited by backticks

        # Associates certain meaningful words with tokens that capture their intent
        KEYWORDS = {
            **Tokenizer.KEYWORDS,
            "INT64": TokenType.BIGINT,
            "FLOAT64": TokenType.DOUBLE,
        }

    class Generator(Generator):
        # Specifies how AST nodes, i.e. subclasses of exp.Expression, should be converted into SQL
        TRANSFORMS = {
            exp.Array: lambda self, e: f"[{self.expressions(e)}]",
        }

        # Specifies how AST nodes representing data types should be converted into SQL
        TYPE_MAPPING = {
            exp.DataType.Type.TINYINT: "INT64",
            exp.DataType.Type.SMALLINT: "INT64",
            ...
        }
```

The above example demonstrates how certain parts of the base Dialect class can be overridden to match a different specification. Even though it is a fairly realistic starting point, we strongly encourage the reader to study existing dialect implementations in order to understand how their various components can be modified, depending on the use-case.


## Column Level Lineage
Column-level lineage (CLL) refers to tracking the flow and transformation of data from source columns to destination columns throughout the execution of SQL queries. It is an important aspect of data lineage that helps in understanding how data is derived, transformed, and used across different parts of a data pipeline or database system.

SQLGlot offers an implementation of CLL in `lineage.py`. An example of it's use case:


```Python
import sqlglot
from sqlglot.lineage import lineage

node = lineage(
    column="a",
    sql="WITH z AS (SELECT a FROM y) SELECT a FROM z",
    schema={"x": {"a": "int"}},
    sources={"y": "SELECT * FROM x"},
)
```

### Implementation details
In the previous example, we're interested in tracing the origin of column `a`. In order for lineage to work, the following preparatory steps must be carried out:

1. Expand the _sources_ such that we have one standalone query, transforming the original query to:

```SQL
WITH z AS (SELECT a FROM (SELECT * FROM x) AS y) SELECT a FROM z
```

2. Qualify the expanded query:

```SQL
WITH z AS (SELECT y.a FROM (SELECT x.a FROM x AS x) AS y) SELECT z.a FROM z AS z
```

With these steps the query is brought into a canonicalized form and the column level lineage for `a` can be tracked. This is a top-down process, meaning that we'd start from `z.a` and traverse the subqueries to trace it's origin which is `y.a`. Similarly, we find that `y.a` is the result of `x.a`'s projection.

Once the inner-most scope has been reached and the expanded sources are exhausted, the `schema` will be searched to validate that the final tables (e.g `x` in this case) is a valid table and `a` is a columned defined in it as well.

At each step, the column of interest (i.e `z.a`, `y.a` etc) is wrapped in a lineage object `Node` that is linked to it's upstream (or parent) nodes; This incrementally builts into a linked-list of `Nodes` that traces the lineage backwards for the target column such as `x.a -> y.a -> z.a`. The result can be visualized using the `node.to_html()` function which outputs an HTML graph based on vis.js.